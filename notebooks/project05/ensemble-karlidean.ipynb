{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab40940a",
   "metadata": {},
   "source": [
    "# Project 5 - Ensemble\n",
    "**Author:** Karli Dean\\\n",
    "**Due Date:** November 21, 2025\\\n",
    "**Purpose:** In this Jupyter Notebook, we will analyze the dataset on Wine from the UCI Library. In my sector of the project, we are analyzing the Bagging and AdaBoost models. We will have a final recap explaining what model we would rather use when making a rational decision on this set and topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed8c8f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf369ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a25ee3",
   "metadata": {},
   "source": [
    "## Section 1 - Load and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0280a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset as a DataFrame\n",
    "df = pd.read_csv(\"winequality-red.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65f8433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows to make sure data loaded correctly\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc00fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes per Project Doc\n",
    "# The dataset includes 11 physicochemical input variables (features):\n",
    "# ---------------------------------------------------------------\n",
    "# - fixed acidity          mostly tartaric acid\n",
    "# - volatile acidity       mostly acetic acid (vinegar)\n",
    "# - citric acid            can add freshness and flavor\n",
    "# - residual sugar         remaining sugar after fermentation\n",
    "# - chlorides              salt content\n",
    "# - free sulfur dioxide    protects wine from microbes\n",
    "# - total sulfur dioxide   sum of free and bound forms\n",
    "# - density                related to sugar content\n",
    "# - pH                     acidity level (lower = more acidic)\n",
    "# - sulphates              antioxidant and microbial stabilizer\n",
    "# - alcohol                % alcohol by volume\n",
    "\n",
    "# The target variable is:\n",
    "# - quality (integer score from 0 to 10, rated by wine tasters)\n",
    "\n",
    "# We will simplify this target into three categories:\n",
    "#   - low (3–4), medium (5–6), high (7–8) to make classification feasible.\n",
    "#   - we will also make this numeric (we want both for clarity)\n",
    "# The dataset contains 1599 samples and 12 columns (11 features + target)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9938bbc6",
   "metadata": {},
   "source": [
    "## Section 2 - Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763451e2",
   "metadata": {},
   "source": [
    "We're going to next prepare the data for our analysis. There are descriptions after each piece for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f769af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function that:\n",
    "\n",
    "# Takes one input, the quality (which we will temporarily name q while in the function)\n",
    "# And returns a string of the quality label (low, medium, high)\n",
    "# This function will be used to create the quality_label column\n",
    "def quality_to_label(q):\n",
    "    if q <= 4:\n",
    "        return \"low\"\n",
    "    elif q <= 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "# Call the apply() method on the quality column to create the new quality_label column\n",
    "df[\"quality_label\"] = df[\"quality\"].apply(quality_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c04d0e",
   "metadata": {},
   "source": [
    "### Why Convert Quality Scores into Labels?\n",
    "\n",
    "The original `quality` variable is numeric, which is useful for calculations but not always intuitive to understand. To make the analysis more meaningful, I created the `quality_to_label()` function to group quality scores into three categories: **low**, **medium**, and **high**. \n",
    "\n",
    "This transformation is helpful because:\n",
    "\n",
    "- It makes the data more interpretable for humans.\n",
    "- It allows us to analyze trends by category instead of raw numbers.\n",
    "- It improves visualizations such as bar charts and comparisons across groups.\n",
    "- It supports slicing, grouping, and summarizing the data more easily.\n",
    "\n",
    "By applying this function, we add a new column (`quality_label`) that captures the same information in a clearer and more useful format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442d8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, create a numeric column for modeling: 0 = low, 1 = medium, 2 = high\n",
    "def quality_to_number(q):\n",
    "    if q <= 4:\n",
    "        return 0\n",
    "    elif q <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Creating the df based on this\n",
    "df[\"quality_numeric\"] = df[\"quality\"].apply(quality_to_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f51d16",
   "metadata": {},
   "source": [
    "### Why Convert Quality Labels into Numbers?\n",
    "\n",
    "Some machine learning models work best with numeric features rather than text labels. To prepare the quality information for modeling, I created the `quality_to_number()` function, which maps:\n",
    "\n",
    "- low → 0  \n",
    "- medium → 1  \n",
    "- high → 2  \n",
    "\n",
    "This gives us a simple, ordinal numeric feature that preserves the order of the categories while making it suitable for ML algorithms, correlations, and statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a37116",
   "metadata": {},
   "source": [
    "## Section 3 - Feature Selection and Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "096d128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features (X) and target (y)\n",
    "# Features: all columns except 'quality' and 'quality_label' and 'quality_numberic' - drop these from the input array\n",
    "# Target: quality_label (the new column we just created)\n",
    "X = df.drop(columns=[\"quality\", \"quality_label\", \"quality_numeric\"])  # Features\n",
    "y = df[\"quality_numeric\"]  # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c7095",
   "metadata": {},
   "source": [
    "### Why these features?\n",
    "\n",
    "- The original quality column is the raw human rating and is being replaced by engineered versions, so I dropped it from the feature set.\n",
    "\n",
    "- `quality_label` is a descriptive/categorical version of the target that I use for interpretation and plotting, but including it as an input feature would leak the answer into the model, so it is also dropped.\n",
    "\n",
    "- `quality_numeric` is the numeric encoding of wine quality (0 = low, 1 = medium, 2 = high), and this is what I want the model to predict, so it belongs in y, not in X.\n",
    "\n",
    "- By dropping [\"quality\", \"quality_label\", \"quality_numeric\"], I ensure that:\n",
    "\n",
    "- X contains only the 11 physicochemical features (fixed acidity, volatile acidity, citric acid, etc.).\n",
    "\n",
    "- y contains a clean, numeric target that is suitable for classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeeca4a",
   "metadata": {},
   "source": [
    "## Section 4 - Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa53a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (stratify to preserve class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e6f05e",
   "metadata": {},
   "source": [
    "## Section 5 - Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70811f61",
   "metadata": {},
   "source": [
    "### Model Evaluation Approach\n",
    "\n",
    "To keep the evaluation process consistent across all models, I created a helper function `evaluate_model()` that:\n",
    "\n",
    "- trains the model  \n",
    "- generates predictions for both the training and test sets  \n",
    "- computes the accuracy and F1 scores  \n",
    "- prints a confusion matrix for the test set  \n",
    "- stores all results in a list for later comparison  \n",
    "\n",
    "Using a helper function ensures that each model is evaluated using the exact same metrics and structure, which makes the results fair and easy to compare.\n",
    "\n",
    "### Why Accuracy and F1 Score?\n",
    "\n",
    "I used **accuracy** to measure overall predictive performance, but accuracy alone can be misleading when classes are imbalanced. Because the wine quality dataset has more samples in the “medium” class, I also included the **weighted F1 score**, which accounts for precision and recall while adjusting for class frequency. This gives a more reliable view of how well each model performs across all classes.\n",
    "\n",
    "### Why Compare AdaBoost and Bagging?\n",
    "\n",
    "AdaBoost and Bagging are both ensemble methods but they operate very differently:\n",
    "\n",
    "- **Bagging (Bootstrap Aggregation)** trains many base learners independently on different bootstrapped samples. It reduces variance and improves stability, especially for decision trees.\n",
    "- **AdaBoost (Adaptive Boosting)** trains learners sequentially, giving more weight to samples the previous models misclassified. It focuses on reducing bias and improving difficult cases.\n",
    "\n",
    "Comparing these two approaches shows how different ensemble strategies behave on the same dataset.\n",
    "\n",
    "### Tracking Results\n",
    "\n",
    "I created `results1` and `results2` as lists to store the performance metrics for each model. The helper function appends a dictionary containing:\n",
    "\n",
    "- Model name  \n",
    "- Train accuracy  \n",
    "- Test accuracy  \n",
    "- Train F1  \n",
    "- Test F1  \n",
    "\n",
    "This allows me to summarize model performance later in a results table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40347aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train and evaluate models\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, results):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n{name} Results\")\n",
    "    print(\"Confusion Matrix (Test):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results = [results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Train Accuracy\": train_acc,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Train F1\": train_f1,\n",
    "            \"Test F1\": test_f1,\n",
    "        }\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "443395b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoost (100) Results\n",
      "Confusion Matrix (Test):\n",
      "[[  1  12   0]\n",
      " [  5 240  19]\n",
      " [  0  20  23]]\n",
      "Train Accuracy: 0.8342, Test Accuracy: 0.8250\n",
      "Train F1 Score: 0.8209, Test F1 Score: 0.8158\n"
     ]
    }
   ],
   "source": [
    "# Calling results\n",
    "results1 = []\n",
    "\n",
    "# 3. AdaBoost\n",
    "evaluate_model(\n",
    "    \"AdaBoost (100)\",\n",
    "    AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04e413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging (DT, 100) Results\n",
      "Confusion Matrix (Test):\n",
      "[[  0  13   0]\n",
      " [  0 252  12]\n",
      " [  0  12  31]]\n",
      "Train Accuracy: 1.0000, Test Accuracy: 0.8844\n",
      "Train F1 Score: 1.0000, Test F1 Score: 0.8655\n"
     ]
    }
   ],
   "source": [
    "# Calling results\n",
    "results2 = []\n",
    "\n",
    "# 8. Bagging\n",
    "evaluate_model(\n",
    "    \"Bagging (DT, 100)\",\n",
    "    BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42\n",
    "    ),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479237b",
   "metadata": {},
   "source": [
    "## Section 6 - Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f2dfd",
   "metadata": {},
   "source": [
    "### Summary of Model Performance\n",
    "\n",
    "After evaluating each model using the `evaluate_model()` function, I combined the individual results lists into a single DataFrame. This summary table allows me to directly compare performance across all models in terms of both **accuracy** and **F1 score** on the training and test sets.\n",
    "\n",
    "Using one consolidated table makes it much easier to:\n",
    "\n",
    "- identify which model generalizes best,\n",
    "- spot overfitting (high train score but lower test score),\n",
    "- compare performance across ensemble methods, and\n",
    "- determine whether the more complex model (e.g., AdaBoost) actually outperforms simpler approaches (e.g., Bagging).\n",
    "\n",
    "This summary DataFrame provides a clear side-by-side comparison that supports my final model selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "916aab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(results1)\n",
    "df2 = pd.DataFrame(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f15f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac74b9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of All Models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost (100)</td>\n",
       "      <td>0.834246</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.820863</td>\n",
       "      <td>0.815803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging (DT, 100)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.865452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Train Accuracy  Test Accuracy  Train F1   Test F1\n",
       "0     AdaBoost (100)        0.834246       0.825000  0.820863  0.815803\n",
       "1  Bagging (DT, 100)        1.000000       0.884375  1.000000  0.865452"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nSummary of All Models:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8380e464",
   "metadata": {},
   "source": [
    "## Section 7 - Conclusions and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13039ca",
   "metadata": {},
   "source": [
    "### Interpretation of Model Performances\n",
    "\n",
    "What stands out immediately is how differently the two ensemble methods behave.  \n",
    "The Bagging model shows **extremely high training performance (100% accuracy/F1)**, which is typical for Decision Tree–based ensembles. Decision Trees can easily memorize the training data, and Bagging does not attempt to reduce that tendency—its strength lies in lowering variance through averaging, not preventing overfitting. \n",
    "\n",
    "Despite perfectly fitting the training set, the Bagging model still generalizes well, achieving **about 88% accuracy on the test set**, which is strong for this dataset.\n",
    "\n",
    "AdaBoost behaves very differently. Because it focuses on reducing bias and sequentially correcting mistakes, it does not fully memorize the training set. This results in training and testing accuracies that are much closer (around 82%). AdaBoost is more stable and less prone to extreme overfitting, but in this case it does not achieve the same test performance as Bagging.\n",
    "\n",
    "**Overall, Bagging performs best** on this dataset. Even though it overfits the training data, its test accuracy and F1 score are higher, indicating that it captures the underlying patterns more effectively. If I were making predictions or drawing insights, I would rely on the Bagging (Decision Tree) ensemble because it provides stronger real-world performance on unseen data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-ml-karlidean2 (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
